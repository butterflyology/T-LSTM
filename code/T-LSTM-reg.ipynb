{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T-LSTM for regression\n",
    "\n",
    "Modified from Baytas 2017's unsupervised learning [project](https://github.com/illidanlab/T-LSTM/blob/master/TLSTM.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 60\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to write out the specific parts of the T-LSTM model and their equivalent in code:\n",
    "\n",
    "- Short-term memory\n",
    "$$C^{S}_{t-1} =  \\textrm{tanh}(W_{d}C_{t-1} + b_{d})$$\n",
    "`C_ST = tf.nn.tanh(tf.matmul(prev_cell, self.W_decomp) + self.b_decomp)`\n",
    "\n",
    "\n",
    "- Discounted short-term memory\n",
    "$$\\hat{C}^{S}_{t-1} = C^{S}_{t-1} \\times g(\\Delta{t})$$\n",
    "`C_ST_dis = tf.multiply(T, C_ST)` - Where `T` is elapsed time\n",
    "\n",
    "\n",
    "- Long-term memory\n",
    "$$C^{T}_{t-1} = C_{t-1} + C^{S}_{t-1}$$\n",
    "\n",
    "\n",
    "- Adjusted previous memory\n",
    "$$C^{*}_{t-1} = C^{T}_{t-1} + \\hat{C}^{S}_{t-1}$$\n",
    "\n",
    "\n",
    "- Forget gate\n",
    "$$f_{t} = \\sigma(W_{f}x_{t} + U_{f}h_{t-1} + b_{f})$$\n",
    "    `f = tf.sigmoid(tf.matmul(x, self.Wf) + tf.matmul(prev_hidden_state, self.Uf) + self.bf)`\n",
    "\n",
    "\n",
    "- Input gate\n",
    "$$i_{t} = \\sigma(W_{i}x_{t} + U_{i}h_{t-1} + b_{i})$$\n",
    "`i = tf.sigmoid(tf.matmul(x, self.Wi) + tf.matmul(prev_hidden_state, self.Ui) + self.bi)`\n",
    "\n",
    "\n",
    "- Output gate\n",
    "$$o_{t} = \\sigma(W_{o}x_{t} + U_{o}h_{t-1} + b_{o})$$\n",
    "`o = tf.sigmoid(tf.matmul(x, self.Wog) + tf.matmul(prev_hidden_state, self.Uog) + self.bog)`\n",
    "\n",
    "\n",
    "- Candidate memory\n",
    "$$\\tilde{C} = \\textrm{tanh}(W_{c}x_{t} + U_{c}h_{t-1} + b_{c}$$\n",
    "`C = tf.nn.tanh(tf.matmul(x, self.Wc) + tf.matmul(prev_hidden_state, self.Uc) + self.bc)`\n",
    "\n",
    "\n",
    "- Current memory\n",
    "$$C_{t} = f_{t} \\times C^{*}_{t-1} + i_{t} \\times \\tilde{C}$$\n",
    "`Ct = f * prev_cell + i * C`\n",
    "\n",
    "\n",
    "- Current hidden state\n",
    "$$h_{t} = o \\times \\textrm{tanh}(C_{t})$$\n",
    "`current_hidden_state = o * tf.nn.tanh(Ct)`\n",
    "\n",
    "\n",
    "**Where**:\n",
    "\n",
    "- $x_{t}$ represents the current input\n",
    "- $h_{t-1}$ and $h_{t}$ are the previous and current hidden states \n",
    "- $C_{t-1}$ and $C_{t}$ are the previous and current cell memories\n",
    "- $\\{W_{f}, U_{f}, b_{f}\\}$ - forget gate network parameters\n",
    "- $\\{W_{i}, U_{i}, b_{i}\\}$ - input gate network parameters\n",
    "- $\\{W_{o}, U_{o}, b_{o}\\}$ - output gate network parameters\n",
    "- $\\{W_{c}, U_{c}, b_{c}\\}$ - candidate memory network parameters\n",
    "- $\\{W_{d}, b_{d}\\}$ - subspace decomposition network parameters\n",
    "- $\\Delta_{t}$ - elapsed time between $x_{t-1}$ and $x_{t}$\n",
    "- $g(\\cdot)$ is the decay function\n",
    "    - $g(\\Delta_{t}) = \\frac{1}{\\Delta_{t}}$ - small amount of elapsed time\n",
    "    - $g(\\Delta_{t}) = \\frac{1}{\\textrm{log}(e + \\Delta_{t})}$ - large amount of elapsed time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\{W_{f}, U_{f}, b_{f}\\}$ - forget gate network parameters\n",
    "    - $W_{f}$ - `self.Wf = self.init_weights(self.input_dim, self.hidden_dim, name = 'Forget_Hidden_weight',reg = None)`\n",
    "    - $U_{f}$ - `self.Uf = self.init_weights(self.hidden_dim, self.hidden_dim, name = 'Forget_State_weight',reg = None)`\n",
    "    - $b_{f}$ - `self.bf = self.init_bias(self.hidden_dim, name = 'Forget_Hidden_bias')`\n",
    "\n",
    "\n",
    "- $\\{W_{i}, U_{i}, b_{i}\\}$ - input gate network parameters\n",
    "    - $W_{i}$ - `self.Wi = self.init_weights(self.input_dim, self.hidden_dim, name = 'Input_Hidden_weight', reg = None)`\n",
    "    - $U_{i}$ - `self.Ui = self.init_weights(self.hidden_dim, self.hidden_dim, name = 'Input_State_weight',reg = None)`\n",
    "    - $b_{i}$ - `self.bi = self.init_bias(self.hidden_dim, name = 'Input_Hidden_bias')`\n",
    "\n",
    "\n",
    "- $\\{W_{o}, U_{o}, b_{o}\\}$ - output gate network parameters\n",
    "    - $W_{o}$ - `self.Wog = self.init_weights(self.input_dim, self.hidden_dim, name = 'Output_Hidden_weight',reg = None)`\n",
    "    - $U_{o}$ - `self.Uog = self.init_weights(self.hidden_dim, self.hidden_dim, name = 'Output_State_weight',reg = None)`\n",
    "    - $b_{i}$ - `self.bog = self.init_bias(self.hidden_dim, name = 'Output_Hidden_bias')`\n",
    "    \n",
    "    \n",
    "- $\\{W_{c}, U_{c}, b_{c}\\}$ - candidate memory network parameters\n",
    "    - $W_{c}$ - `self.Wc = self.init_weights(self.input_dim, self.hidden_dim, name = 'Cell_Hidden_weight',reg = None)`\n",
    "    - $U_{c}$ - `self.Uc = self.init_weights(self.hidden_dim, self.hidden_dim, name = 'Cell_State_weight',reg = None)`\n",
    "    - $b_{i}$ - `self.bc = self.init_bias(self.hidden_dim, name = 'Cell_Hidden_bias')`\n",
    "    \n",
    "    \n",
    "- $\\{W_{d}, b_{d}\\}$ - subspace decomposition network parameters\n",
    "    - $W_{d}$ - `self.W_decomp = self.no_init_weights(self.hidden_dim, self.hidden_dim, name = 'Decomposition_Hidden_weight')`\n",
    "    - $b_{d}$ - `self.b_decomp = self.no_init_bias(self.hidden_dim, name = 'Decomposition_Hidden_bias_enc')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TLSTM(object):\n",
    "    def init_weights(self, input_dim, output_dim, name, std = 0.1, reg = None):\n",
    "        return tf.get_variable(name, shape = [input_dim, output_dim], initializer = tf.random_normal_initializer(0.0, std), regularizer = reg)\n",
    "\n",
    "    \n",
    "    def init_bias(self, output_dim, name):\n",
    "        return tf.get_variable(name, shape = [output_dim], initializer = tf.constant_initializer(1.0))\n",
    "\n",
    "    \n",
    "    def no_init_weights(self, input_dim, output_dim, name):\n",
    "        return tf.get_variable(name,shape = [input_dim, output_dim])\n",
    "\n",
    "    \n",
    "    def no_init_bias(self, output_dim, name):\n",
    "        return tf.get_variable(name,shape = [output_dim])\n",
    "\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, fc_dim,train):\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # [batch size x seq length x input dim]\n",
    "        self.input = tf.placeholder('float', shape = [None, None, self.input_dim])\n",
    "        self.labels = tf.placeholder('float', shape = [None, output_dim])\n",
    "        self.time = tf.placeholder('float', shape = [None, None])\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "        if train == 1:\n",
    "\n",
    "            # forget gate network parameters\n",
    "            self.Wf = self.init_weights(self.input_dim, self.hidden_dim, name = 'Forget_Hidden_weight',reg = None)\n",
    "            self.Uf = self.init_weights(self.hidden_dim, self.hidden_dim, name = 'Forget_State_weight',reg = None)\n",
    "            self.bf = self.init_bias(self.hidden_dim, name = 'Forget_Hidden_bias')\n",
    "            \n",
    "            # input gate network parameters\n",
    "            self.Wi = self.init_weights(self.input_dim, self.hidden_dim, name = 'Input_Hidden_weight', reg = None)\n",
    "            self.Ui = self.init_weights(self.hidden_dim, self.hidden_dim, name = 'Input_State_weight',reg = None)\n",
    "            self.bi = self.init_bias(self.hidden_dim, name = 'Input_Hidden_bias')\n",
    "\n",
    "            # output gate network parameters\n",
    "            self.Wog = self.init_weights(self.input_dim, self.hidden_dim, name = 'Output_Hidden_weight',reg = None)\n",
    "            self.Uog = self.init_weights(self.hidden_dim, self.hidden_dim, name = 'Output_State_weight',reg = None)\n",
    "            self.bog = self.init_bias(self.hidden_dim, name = 'Output_Hidden_bias')\n",
    "\n",
    "            # candidate memory network parameters\n",
    "            self.Wc = self.init_weights(self.input_dim, self.hidden_dim, name = 'Cell_Hidden_weight',reg = None)\n",
    "            self.Uc = self.init_weights(self.hidden_dim, self.hidden_dim, name = 'Cell_State_weight',reg = None)\n",
    "            self.bc = self.init_bias(self.hidden_dim, name = 'Cell_Hidden_bias')\n",
    "\n",
    "            # subspace decomposition network parameters\n",
    "            self.W_decomp = self.init_weights(self.hidden_dim, self.hidden_dim, name = 'Decomposition_Hidden_weight',reg = None)\n",
    "            self.b_decomp = self.init_bias(self.hidden_dim, name = 'Decomposition_Hidden_bias_enc')\n",
    "\n",
    "            # not sure\n",
    "            self.Wo = self.init_weights(self.hidden_dim, fc_dim, name = 'Fc_Layer_weight', reg = None) #tf.contrib.layers.l2_regularizer(scale = 0.001)\n",
    "            self.bo = self.init_bias(fc_dim, name = 'Fc_Layer_bias')\n",
    "\n",
    "            # softmax for classification\n",
    "            self.W_softmax = self.init_weights(fc_dim, output_dim, name = 'Output_Layer_weight', reg = None)#tf.contrib.layers.l2_regularizer(scale=0.001)\n",
    "            self.b_softmax = self.init_bias(output_dim, name = 'Output_Layer_bias')\n",
    "\n",
    "        else:\n",
    "\n",
    "            # forget gate network parameters\n",
    "            self.Wf = self.no_init_weights(self.input_dim, self.hidden_dim, name = 'Forget_Hidden_weight')\n",
    "            self.Uf = self.no_init_weights(self.hidden_dim, self.hidden_dim, name = 'Forget_State_weight')\n",
    "            self.bf = self.no_init_bias(self.hidden_dim, name = 'Forget_Hidden_bias')\n",
    "            \n",
    "            # input gate network parameters\n",
    "            self.Wi = self.no_init_weights(self.input_dim, self.hidden_dim, name = 'Input_Hidden_weight')\n",
    "            self.Ui = self.no_init_weights(self.hidden_dim, self.hidden_dim, name = 'Input_State_weight')\n",
    "            self.bi = self.no_init_bias(self.hidden_dim, name = 'Input_Hidden_bias')\n",
    "\n",
    "            # output gate network parameters\n",
    "            self.Wog = self.no_init_weights(self.input_dim, self.hidden_dim, name = 'Output_Hidden_weight')\n",
    "            self.Uog = self.no_init_weights(self.hidden_dim, self.hidden_dim, name = 'Output_State_weight')\n",
    "            self.bog = self.no_init_bias(self.hidden_dim, name = 'Output_Hidden_bias')\n",
    "\n",
    "            # candidate memory network parameters\n",
    "            self.Wc = self.no_init_weights(self.input_dim, self.hidden_dim, name = 'Cell_Hidden_weight')\n",
    "            self.Uc = self.no_init_weights(self.hidden_dim, self.hidden_dim, name = 'Cell_State_weight')\n",
    "            self.bc = self.no_init_bias(self.hidden_dim, name = 'Cell_Hidden_bias')\n",
    "\n",
    "            # subspace decomposition network parameters\n",
    "            self.W_decomp = self.no_init_weights(self.hidden_dim, self.hidden_dim, name = 'Decomposition_Hidden_weight')\n",
    "            self.b_decomp = self.no_init_bias(self.hidden_dim, name = 'Decomposition_Hidden_bias_enc')\n",
    "\n",
    "            # not sure\n",
    "            self.Wo = self.no_init_weights(self.hidden_dim, fc_dim, name = 'Fc_Layer_weight')\n",
    "            self.bo = self.no_init_bias(fc_dim, name = 'Fc_Layer_bias')\n",
    "\n",
    "            # softmax for classification\n",
    "            self.W_softmax = self.no_init_weights(fc_dim, output_dim, name = 'Output_Layer_weight')\n",
    "            self.b_softmax = self.no_init_bias(output_dim, name = 'Output_Layer_bias')\n",
    "\n",
    "\n",
    "    def TLSTM_Unit(self, prev_hidden_memory, concat_input):\n",
    "        prev_hidden_state, prev_cell = tf.unstack(prev_hidden_memory)\n",
    "\n",
    "        batch_size = tf.shape(concat_input)[0]\n",
    "        x = tf.slice(concat_input, [0, 1], [batch_size, self.input_dim])\n",
    "        t = tf.slice(concat_input, [0, 0], [batch_size, 1])\n",
    "\n",
    "        # Dealing with time irregularity\n",
    "\n",
    "        # Map elapse time in days or months\n",
    "        T = self.map_elapse_time(t)\n",
    "\n",
    "        # Decompose the previous cell if there is a elapse time\n",
    "        C_ST = tf.nn.tanh(tf.matmul(prev_cell, self.W_decomp) + self.b_decomp)\n",
    "        C_ST_dis = tf.multiply(T, C_ST)\n",
    "        # if T is 0, then the weight is one\n",
    "        prev_cell = prev_cell - C_ST + C_ST_dis\n",
    "        \n",
    "\n",
    "        # Forget Gate\n",
    "        f = tf.sigmoid(tf.matmul(x, self.Wf) + tf.matmul(prev_hidden_state, self.Uf) + self.bf)\n",
    "        \n",
    "        # Input gate\n",
    "        i = tf.sigmoid(tf.matmul(x, self.Wi) + tf.matmul(prev_hidden_state, self.Ui) + self.bi)\n",
    "       \n",
    "        # Output Gate\n",
    "        o = tf.sigmoid(tf.matmul(x, self.Wog) + tf.matmul(prev_hidden_state, self.Uog) + self.bog)\n",
    "\n",
    "        # Candidate Memory Cell\n",
    "        C = tf.nn.tanh(tf.matmul(x, self.Wc) + tf.matmul(prev_hidden_state, self.Uc) + self.bc)\n",
    "\n",
    "        # Current Memory cell\n",
    "        Ct = f * prev_cell + i * C\n",
    "\n",
    "        # Current Hidden state\n",
    "        current_hidden_state = o * tf.nn.tanh(Ct)\n",
    "\n",
    "        return tf.stack([current_hidden_state, Ct])\n",
    "\n",
    "    \n",
    "    def get_states(self): # Returns all hidden states for the samples in a batch\n",
    "        batch_size = tf.shape(self.input)[0]\n",
    "        scan_input_ = tf.transpose(self.input, perm = [2, 0, 1])\n",
    "        scan_input = tf.transpose(scan_input_) #scan input is [seq_length x batch_size x input_dim]\n",
    "        scan_time = tf.transpose(self.time) # scan_time [seq_length x batch_size]\n",
    "        initial_hidden = tf.zeros([batch_size, self.hidden_dim], tf.float32)\n",
    "        ini_state_cell = tf.stack([initial_hidden, initial_hidden])\n",
    "        # make scan_time [seq_length x batch_size x 1]\n",
    "        scan_time = tf.reshape(scan_time, [tf.shape(scan_time)[0],tf.shape(scan_time)[1],1])\n",
    "        concat_input = tf.concat([scan_time, scan_input],2) # [seq_length x batch_size x input_dim+1]\n",
    "        packed_hidden_states = tf.scan(self.TLSTM_Unit, concat_input, initializer=ini_state_cell, name = 'states')\n",
    "        all_states = packed_hidden_states[:, 0, :, :]\n",
    "        return all_states\n",
    "\n",
    "    \n",
    "    def get_output(self, state):\n",
    "        output = tf.nn.relu(tf.matmul(state, self.Wo) + self.bo)\n",
    "        output = tf.nn.dropout(output, self.keep_prob)\n",
    "        output = tf.matmul(output, self.W_softmax) + self.b_softmax\n",
    "        return output\n",
    "\n",
    "    def get_outputs(self): # Returns all the outputs\n",
    "        all_states = self.get_states()\n",
    "        all_outputs = tf.map_fn(self.get_output, all_states)\n",
    "        output = tf.reverse(all_outputs, [0])[0, :, :] # Compatible with tensorflow 1.2.1\n",
    "        # output = tf.reverse(all_outputs, [True, False, False])[0, :, :] # Compatible with tensorflow 0.12.1\n",
    "        return output\n",
    "\n",
    "    def get_cost_acc(self):\n",
    "        logits = self.get_outputs()\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.labels, logits=logits))\n",
    "        y_pred = tf.argmax(logits, 1)\n",
    "        y = tf.argmax(self.labels, 1)\n",
    "        return cross_entropy, y_pred, y, logits, self.labels\n",
    "\n",
    "\n",
    "    def map_elapse_time(self, t):\n",
    "\n",
    "        c1 = tf.constant(1, dtype = tf.float32)\n",
    "        c2 = tf.constant(2.7183, dtype = tf.float32)\n",
    "\n",
    "        # T = tf.multiply(self.wt, t) + self.bt\n",
    "\n",
    "        T = tf.div(c1, tf.log(t + c2), name = 'Log_elapse_time')\n",
    "\n",
    "        Ones = tf.ones([1, self.hidden_dim], dtype = tf.float32)\n",
    "\n",
    "        T = tf.matmul(T, Ones)\n",
    "\n",
    "        return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-Aware LSTM\n",
    "# main function for supervised task\n",
    "# An example dataset is shared but the original synthetic dataset\n",
    "# can be accessed from http://www.emrbots.org/.\n",
    "# Inci M. Baytas, 2017\n",
    "#\n",
    "# How to run: Give the correct path to the data\n",
    "# Data is a list where each element is a 3 dimensional matrix which contains same length sequences.\n",
    "# Instead of zero padding, same length sequences are put in the same batch.\n",
    "# Example: L is the list containing all the batches with a length of N.\n",
    "#          L[0].shape gives [number of samples x sequence length x dimensionality]\n",
    "# Please refer the example bash script\n",
    "# You can use Split0 as the data.\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils import shuffle\n",
    "import sys\n",
    "import math\n",
    "import cPickle\n",
    "\n",
    "\n",
    "#from TLSTM import TLSTM\n",
    "\n",
    "def load_pkl(path):\n",
    "    with open(path) as f:\n",
    "        obj = cPickle.load(f)\n",
    "        return obj\n",
    "\n",
    "def convert_one_hot(label_list):\n",
    "    for i in range(len(label_list)):\n",
    "        sec_col = np.ones([label_list[i].shape[0],label_list[i].shape[1],1])\n",
    "        label_list[i] = np.reshape(label_list[i],[label_list[i].shape[0],label_list[i].shape[1],1])\n",
    "        sec_col -= label_list[i]\n",
    "        label_list[i] = np.concatenate([label_list[i],sec_col],2)\n",
    "    return label_list\n",
    "\n",
    "\n",
    "def training(path,learning_rate,training_epochs,train_dropout_prob,hidden_dim,fc_dim,key,model_path):\n",
    "    path_string = path + '/data_train.pkl'\n",
    "    data_train_batches = load_pkl(path_string)\n",
    "\n",
    "    path_string = path + '/elapsed_train.pkl'\n",
    "    elapsed_train_batches = load_pkl(path_string)\n",
    "\n",
    "    path_string = path + '/label_train.pkl'\n",
    "    labels_train_batches = load_pkl(path_string)\n",
    "\n",
    "    path_string = path + '/hidden_ind_train.pkl'\n",
    "    hidden_ind_train = load_pkl(path_string)\n",
    "\n",
    "    number_train_batches = len(data_train_batches)\n",
    "    print(\"Train data is loaded!\")\n",
    "\n",
    "    input_dim = data_train_batches[0].shape[2]\n",
    "    output_dim = labels_train_batches[0].shape[1]\n",
    "\n",
    "    lstm = TLSTM(input_dim, output_dim, hidden_dim, fc_dim,key)\n",
    "\n",
    "    cross_entropy, y_pred, y, logits, labels = lstm.get_cost_acc()\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(training_epochs):  #\n",
    "            # Loop over all batches\n",
    "            total_cost = 0\n",
    "            for i in range(number_train_batches):  #\n",
    "                # batch_xs is [number of patients x sequence length x input dimensionality]\n",
    "                batch_xs, batch_ys, batch_ts = data_train_batches[i], labels_train_batches[i], \\\n",
    "                                                         elapsed_train_batches[i]\n",
    "                batch_ts = np.reshape(batch_ts, [batch_ts.shape[0], batch_ts.shape[2]])\n",
    "                sess.run(optimizer,feed_dict={lstm.input: batch_xs, lstm.labels: batch_ys,\\\n",
    "                                              lstm.keep_prob:train_dropout_prob, lstm.time:batch_ts})\n",
    "\n",
    "\n",
    "        print(\"Training is over!\")\n",
    "        saver.save(sess,model_path)\n",
    "\n",
    "        Y_pred = []\n",
    "        Y_true = []\n",
    "        Labels = []\n",
    "        Logits = []\n",
    "        for i in range(number_train_batches):  #\n",
    "            batch_xs, batch_ys, batch_ts = data_train_batches[i], labels_train_batches[i], \\\n",
    "                                                     elapsed_train_batches[i]\n",
    "            batch_ts = np.reshape(batch_ts, [batch_ts.shape[0], batch_ts.shape[2]])\n",
    "            c_train, y_pred_train, y_train, logits_train, labels_train = sess.run(lstm.get_cost_acc(), feed_dict={\n",
    "                lstm.input:\n",
    "                                                                                                       batch_xs, lstm.labels: batch_ys, \\\n",
    "                                           lstm.keep_prob: train_dropout_prob, lstm.time: batch_ts})\n",
    "\n",
    "            if i > 0:\n",
    "                Y_true = np.concatenate([Y_true, y_train], 0)\n",
    "                Y_pred = np.concatenate([Y_pred, y_pred_train], 0)\n",
    "                Labels = np.concatenate([Labels, labels_train], 0)\n",
    "                Logits = np.concatenate([Logits, logits_train], 0)\n",
    "            else:\n",
    "                Y_true = y_train\n",
    "                Y_pred = y_pred_train\n",
    "                Labels = labels_train\n",
    "                Logits = logits_train\n",
    "\n",
    "        total_acc = accuracy_score(Y_true, Y_pred)\n",
    "        total_auc = roc_auc_score(Labels, Logits, average='micro')\n",
    "        total_auc_macro = roc_auc_score(Labels, Logits, average='macro')\n",
    "        print(\"Train Accuracy = {:.3f}\".format(total_acc))\n",
    "        print(\"Train AUC = {:.3f}\".format(total_auc))\n",
    "        print(\"Train AUC Macro = {:.3f}\".format(total_auc_macro))\n",
    "\n",
    "\n",
    "def testing(path,hidden_dim,fc_dim,key,model_path):\n",
    "    path_string = path + '/data_test.pkl'\n",
    "    data_test_batches = load_pkl(path_string)\n",
    "\n",
    "    path_string = path + '/elapsed_test.pkl'\n",
    "    elapsed_test_batches = load_pkl(path_string)\n",
    "\n",
    "    path_string = path + '/label_test.pkl'\n",
    "    labels_test_batches = load_pkl(path_string)\n",
    "\n",
    "    path_string = path + '/hidden_ind_test.pkl'\n",
    "\n",
    "    number_test_batches = len(data_test_batches)\n",
    "\n",
    "    print(\"Test data is loaded!\")\n",
    "\n",
    "    input_dim = data_test_batches[0].shape[2]\n",
    "    output_dim = labels_test_batches[0].shape[1]\n",
    "\n",
    "    test_dropout_prob = 1.0\n",
    "    lstm_load = TLSTM(input_dim, output_dim, hidden_dim, fc_dim, key)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, model_path)\n",
    "\n",
    "        Y_true = []\n",
    "        Y_pred = []\n",
    "        Logits = []\n",
    "        Labels = []\n",
    "        for i in range(number_test_batches):\n",
    "            batch_xs, batch_ys, batch_ts = data_test_batches[i], labels_test_batches[i], \\\n",
    "                                                     elapsed_test_batches[i]\n",
    "            batch_ts = np.reshape(batch_ts, [batch_ts.shape[0], batch_ts.shape[2]])\n",
    "            c_test, y_pred_test, y_test, logits_test, labels_test = sess.run(lstm_load.get_cost_acc(),\n",
    "                                                                             feed_dict={lstm_load.input: batch_xs,\n",
    "                                                                                        lstm_load.labels: batch_ys,\\\n",
    "                                                                                        lstm_load.time: batch_ts,\\\n",
    "                                                                                        lstm_load.keep_prob: test_dropout_prob})\n",
    "            if i > 0:\n",
    "                Y_true = np.concatenate([Y_true, y_test], 0)\n",
    "                Y_pred = np.concatenate([Y_pred, y_pred_test], 0)\n",
    "                Labels = np.concatenate([Labels, labels_test], 0)\n",
    "                Logits = np.concatenate([Logits, logits_test], 0)\n",
    "            else:\n",
    "                Y_true = y_test\n",
    "                Y_pred = y_pred_test\n",
    "                Labels = labels_test\n",
    "                Logits = logits_test\n",
    "        total_auc = roc_auc_score(Labels, Logits, average='micro')\n",
    "        total_auc_macro = roc_auc_score(Labels, Logits, average='macro')\n",
    "        total_acc = accuracy_score(Y_true, Y_pred)\n",
    "        print(\"Test Accuracy = {:.3f}\".format(total_acc))\n",
    "        print(\"Test AUC Micro = {:.3f}\".format(total_auc))\n",
    "        print(\"Test AUC Macro = {:.3f}\".format(total_auc_macro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '-f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ea5207a16418>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m    \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-ea5207a16418>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(argv)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtraining_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtraining_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '-f'"
     ]
    }
   ],
   "source": [
    "def main(argv):\n",
    "    training_mode = int(sys.argv[1])\n",
    "    path = str(sys.argv[2])\n",
    "\n",
    "    if training_mode == 1:\n",
    "        learning_rate = float(sys.argv[3])\n",
    "        training_epochs = int(sys.argv[4])\n",
    "        dropout_prob = float(sys.argv[5])\n",
    "        hidden_dim = int(sys.argv[6])\n",
    "        fc_dim = int(sys.argv[7])\n",
    "        model_path = str(sys.argv[8])\n",
    "        training(path, learning_rate, training_epochs, dropout_prob, hidden_dim, fc_dim, training_mode, model_path)\n",
    "    else:\n",
    "        hidden_dim = int(sys.argv[3])\n",
    "        fc_dim = int(sys.argv[4])\n",
    "        model_path = str(sys.argv[5])\n",
    "        testing(path, hidden_dim, fc_dim, training_mode, model_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   main(sys.argv[1:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py2_ML",
   "language": "python",
   "name": "py2_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
